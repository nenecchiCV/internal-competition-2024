{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/nenecchiCV/internal-competition-2024/blob/main/method1.ipynb",
      "authorship_tag": "ABX9TyMO9u01UQ2DSsxWeaTS2kkg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "20b666355c5f4d9db77d5bbb2f1830fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_34446a27b1f34093a3618c2ff8c2cc46",
              "IPY_MODEL_9d953ad107ff494ba2bace79c1bce26a"
            ],
            "layout": "IPY_MODEL_33918e9151b946db82bb15263809636c"
          }
        },
        "34446a27b1f34093a3618c2ff8c2cc46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ed2dc4d7e134b21b3f284dd563e277a",
            "placeholder": "​",
            "style": "IPY_MODEL_d59a116b87774435828bf110ccda7ee7",
            "value": "0.030 MB of 0.030 MB uploaded\r"
          }
        },
        "9d953ad107ff494ba2bace79c1bce26a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36713b15649f4b6395f1f8732ce6151f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0f5aae332bbd4786bd6288b797e5d658",
            "value": 1
          }
        },
        "33918e9151b946db82bb15263809636c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ed2dc4d7e134b21b3f284dd563e277a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d59a116b87774435828bf110ccda7ee7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36713b15649f4b6395f1f8732ce6151f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f5aae332bbd4786bd6288b797e5d658": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nenecchiCV/internal-competition-2024/blob/main/method2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-vYPlGg3kAB",
        "outputId": "16a77972-ad2a-4958-f8ee-1bed757e59fc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = \"b846546e2316d4194c74a497599a7b83c9127f07\""
      ],
      "metadata": {
        "id": "zurfTCmEgwqf"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Hy9sI7w5Mg3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad272810-9841-4b3b-f81b-c4d1b7bec798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (0.4.6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        }
      ],
      "source": [
        "!pip install colorama\n",
        "!pip install wandb -qqq\n",
        "import wandb\n",
        "\n",
        "try:\n",
        "    wandb.login(key=api_key)\n",
        "    anonymous = None\n",
        "except:\n",
        "    anonymous = \"must\"\n",
        "    print('To use your W&B account,\\nGo to Add-ons -> Secrets and provide your W&B access token. Use the Label name as WANDB. \\nGet your W&B access token from here: https://wandb.ai/authorize')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "data1_df = pd.read_csv('/content/drive/MyDrive/daicc2024/data_1.csv')\n",
        "data2_df = pd.read_csv('/content/drive/MyDrive/daicc2024/data_2.csv')\n",
        "data_df = pd.concat([data1_df, data2_df], axis=0)\n",
        "data_df.head()\n",
        "\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/daicc2024/test.csv')\n",
        "all_df = pd.concat([data_df, test_df], axis=0)"
      ],
      "metadata": {
        "id": "CeK-NL8tdmWi"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X1 = data_df.drop(columns=[\"id\",'parameter',\"spec\"])\n",
        "x2 = data_df[['spec']]\n",
        "y = data_df[['parameter']]\n",
        "\n",
        "X1_all = all_df.drop(columns=[\"id\",'parameter',\"spec\"])\n",
        "x2_min_max = np.array([[90], [110]]).astype('float')\n",
        "x2_min_max_df = pd.DataFrame(data=x2_min_max, columns=[\"spec\"], dtype=float)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import torch\n",
        "\n",
        "# trainというDataFrameにfit\n",
        "X1_scaler = StandardScaler().fit(X1_all)\n",
        "x2_scaler = MinMaxScaler(feature_range=(-1, 1)).fit(x2_min_max_df)\n",
        "X1_scaled = X1_scaler.transform(X1)\n",
        "x2_scaled = x2_scaler.transform(x2)\n",
        "\n",
        "X1_scaled_tensor = torch.tensor(X1_scaled.reshape(-1, 3, 5), dtype=torch.float64)\n",
        "x2_scaled_tensor = torch.tensor(x2_scaled, dtype=torch.float64)\n",
        "y_tensor = torch.tensor(y.values, dtype=torch.float64)\n",
        "\n",
        "X1_test = test_df.drop(columns=[\"id\"])\n",
        "X1_scaled_test = X1_scaler.transform(X1_test)\n",
        "X1_scaled_test_tensor = torch.tensor(X1_scaled_test.reshape(-1, 3, 5), dtype=torch.float64)"
      ],
      "metadata": {
        "id": "9bxQcjEDgzz8"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "メモ：計測値とスコアをX、パラメータをYとして学習→結果を出すときはスコアを100にする\n",
        "\n",
        "RNN\n",
        "https://qiita.com/MENDY/items/99da56f61f9af51dda15"
      ],
      "metadata": {
        "id": "2PI5v9kKGwq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils import data as data\n",
        "\n",
        "class MyDataset(data.Dataset):\n",
        "    def __init__(self, X1_scaled_tensor, x2_scaled_tensor, y_tensor):\n",
        "        self.X1 = X1_scaled_tensor\n",
        "        self.x2 = x2_scaled_tensor\n",
        "        self.y = y_tensor\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.X1[index], self.x2[index], self.y[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)"
      ],
      "metadata": {
        "id": "xFxsPwmM3oCz"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch_xla\n",
        "# import torch_xla.core.xla_model as xm\n",
        "\n",
        "class CFG:\n",
        "    seed = 99\n",
        "    exp_name = 'exp_1'\n",
        "    comment = exp_name\n",
        "    model_name = 'LSTM-FC'\n",
        "    train_bs = 8\n",
        "    valid_bs = train_bs\n",
        "    data_size = [[3, 5], [1]]\n",
        "    epochs = 200\n",
        "    lr = 2e-4\n",
        "    scheduler = 'CosineAnnealingLR'\n",
        "    min_lr = 1e-6\n",
        "    T_max = int(30000 / train_bs * epochs) + 50\n",
        "    T_0 = 25\n",
        "    warmup_epochs = 0\n",
        "    wd = 1e-6\n",
        "    n_fold = 5\n",
        "    folds = []\n",
        "    device = \"cpu\"\n",
        "    # device = xm.xla_device()\n",
        "    lstm_input_size=5\n",
        "    lstm_hidden_size=10\n",
        "    fc_hidden_size=128\n",
        "    dropout_rate = 0.4\n",
        "#     target_dataset='uwmgi-stride2-dataset/images'"
      ],
      "metadata": {
        "id": "91xKiQmx88ad"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "class LSTMScalarPredictor(nn.Module):\n",
        "    def __init__(self, lstm_input_size=CFG.lstm_input_size, lstm_hidden_size=CFG.lstm_hidden_size, fc_hidden_size=CFG.fc_hidden_size):\n",
        "        super(LSTMScalarPredictor, self).__init__()\n",
        "        self.lstm_hidden_size = lstm_hidden_size\n",
        "        self.lstm = nn.LSTM(lstm_input_size, lstm_hidden_size, batch_first=True)\n",
        "        self.fc1 = nn.Linear(lstm_hidden_size + 1, fc_hidden_size)  # +1 for scalar input\n",
        "        self.fc2 = nn.Linear(fc_hidden_size, fc_hidden_size)\n",
        "        self.fc3 = nn.Linear(fc_hidden_size, 1)\n",
        "        self.dropout1 = nn.Dropout2d(CFG.dropout_rate)\n",
        "        self.dropout2 = nn.Dropout2d(CFG.dropout_rate)\n",
        "        self.dropout3 = nn.Dropout2d(CFG.dropout_rate)\n",
        "        self.bn1 = nn.BatchNorm1d(fc_hidden_size)\n",
        "        self.bn2 = nn.BatchNorm1d(fc_hidden_size)\n",
        "\n",
        "    def forward(self, X1, x2):\n",
        "        # Initialize hidden state with zeros\n",
        "        h0 = torch.zeros(1, X1.size(0), self.lstm_hidden_size).to(X1.device)\n",
        "        c0 = torch.zeros(1, X1.size(0), self.lstm_hidden_size).to(X1.device)\n",
        "\n",
        "        # Forward pass through LSTM layer\n",
        "        lstm_H, _ = self.lstm(X1, (h0, c0))\n",
        "\n",
        "        # Only take the output from the last time step\n",
        "        x = torch.cat([lstm_H[:, -1, :], x2], dim=1)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout3(x)\n",
        "        out = self.fc3(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def init_wandb(self):\n",
        "      for name, param in self.named_parameters():\n",
        "        if 'lstm.weight' in name:\n",
        "            nn.init.xavier_uniform_(param)  # LSTM層の重みをXavier初期化\n",
        "        elif 'fc3.weight' in name:\n",
        "            nn.init.xavier_uniform_(param)  # FC-softmax層の重みをXavier初期化\n",
        "        elif 'fc.weight' in name:\n",
        "            nn.init.kaiming_normal_(param, nonlinearity='relu')  # FC層-reluの重みをHe初期化\n"
      ],
      "metadata": {
        "id": "IEHjxyGTLrpq"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MSELoss = nn.MSELoss()\n",
        "def criterion(y_pred, y_true):\n",
        "    return MSELoss(y_pred, y_true)"
      ],
      "metadata": {
        "id": "aA-i_sAAMklM"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import os\n",
        "def set_seed(seed = 42):\n",
        "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
        "    This is for REPRODUCIBILITY.'''\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # When running on the CuDNN backend, two further options must be set\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    # Set a fixed value for the hash seed\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    print('> SEEDING DONE')\n",
        "\n",
        "set_seed(CFG.seed)"
      ],
      "metadata": {
        "id": "7yMWT0eH8Z-E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ae602a9-7092-488e-d05b-a6c5a07fe13c"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> SEEDING DONE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import gc\n",
        "\n",
        "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
        "    model.train()\n",
        "\n",
        "    dataset_size = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Train ')\n",
        "    for step, (X1, x2, y_true) in pbar:\n",
        "        X1 = X1.to(device, dtype=torch.float)\n",
        "        x2  = x2.to(device, dtype=torch.float)\n",
        "        y_true  = y_true.to(device, dtype=torch.float)\n",
        "\n",
        "        batch_size = X1.size(0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_pred = model(X1, x2)\n",
        "        loss = criterion(y_pred, y_true)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        running_loss += (loss.item() * batch_size)\n",
        "        dataset_size += batch_size\n",
        "\n",
        "        epoch_loss = running_loss / dataset_size\n",
        "\n",
        "        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        pbar.set_postfix(train_loss=f'{epoch_loss:0.4f}',\n",
        "                        lr=f'{current_lr:0.5f}',\n",
        "                        gpu_mem=f'{mem:0.2f} GB')\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return epoch_loss\n",
        "\n",
        "@torch.no_grad()\n",
        "def valid_one_epoch(model, dataloader, device, epoch):\n",
        "    model.eval()\n",
        "\n",
        "    dataset_size = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    val_scores = []\n",
        "\n",
        "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Valid ')\n",
        "    for step, (X1, x2, y_true) in pbar:\n",
        "        X1 = X1.to(device, dtype=torch.float)\n",
        "        x2 = x2.to(device, dtype=torch.float)\n",
        "        y_true  = y_true.to(device, dtype=torch.float)\n",
        "\n",
        "        batch_size = X1.size(0)\n",
        "\n",
        "        y_pred = model(X1, x2)\n",
        "        loss = criterion(y_pred, y_true)\n",
        "\n",
        "        running_loss += (loss.item() * batch_size)\n",
        "        dataset_size += batch_size\n",
        "\n",
        "        epoch_loss = running_loss / dataset_size\n",
        "\n",
        "        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        pbar.set_postfix(valid_loss=f'{epoch_loss:0.4f}',\n",
        "                        lr=f'{current_lr:0.5f}',\n",
        "                        gpu_memory=f'{mem:0.2f} GB')\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return epoch_loss"
      ],
      "metadata": {
        "id": "w70KIUNFKALD"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "from collections import defaultdict\n",
        "import time\n",
        "from colorama import Fore, Back, Style\n",
        "c_  = Fore.GREEN\n",
        "sr_ = Style.RESET_ALL\n",
        "\n",
        "def run_training(model, optimizer, scheduler, device, num_epochs):\n",
        "    # To automatically log gradients\n",
        "    wandb.watch(model, log_freq=100)\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"cuda: {}\\n\".format(torch.cuda.get_device_name()))\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss      = np.inf\n",
        "    best_epoch     = -1\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        gc.collect()\n",
        "        print(f'Epoch {epoch}/{num_epochs}', end='')\n",
        "        train_loss = train_one_epoch(model, optimizer, scheduler,\n",
        "                                           dataloader=train_loader,\n",
        "                                           device=CFG.device, epoch=epoch)\n",
        "\n",
        "        val_loss = valid_one_epoch(model, valid_loader,\n",
        "                                                 device=CFG.device,\n",
        "                                                 epoch=epoch)\n",
        "\n",
        "        history['Train Loss'].append(train_loss)\n",
        "        history['Valid Loss'].append(val_loss)\n",
        "\n",
        "        # Log the metrics\n",
        "        wandb.log({\"Train Loss\": train_loss,\n",
        "                   \"Valid Loss\": val_loss,\n",
        "                   \"LR\":scheduler.get_last_lr()[0]})\n",
        "\n",
        "        if val_loss < best_loss:\n",
        "            print(f\"{c_}Valid Score Improved ({best_loss:0.4f} ---> {val_loss:0.4f})\")\n",
        "            best_loss = val_loss\n",
        "            best_epoch   = epoch\n",
        "            run.summary[\"Best Loss\"]    = best_loss\n",
        "            run.summary[\"Best Epoch\"]   = best_epoch\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            PATH = f\"best_epoch-{fold:02d}.bin\"\n",
        "            torch.save(model.state_dict(), PATH)\n",
        "            # Save a model file from the current directory\n",
        "            wandb.save(PATH)\n",
        "            print(f\"Model Saved{sr_}\")\n",
        "\n",
        "\n",
        "        print(); print()\n",
        "\n",
        "    end = time.time()\n",
        "    time_elapsed = end - start\n",
        "    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
        "    print(\"Best Loss: {:.4f}\".format(best_loss))\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "xUe04P51LcW3"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_scheduler(optimizer):\n",
        "    if CFG.scheduler == 'CosineAnnealingLR':\n",
        "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CFG.T_max,\n",
        "                                                   eta_min=CFG.min_lr)\n",
        "    elif CFG.scheduler == 'CosineAnnealingWarmRestarts':\n",
        "        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CFG.T_0,\n",
        "                                                             eta_min=CFG.min_lr)\n",
        "    elif CFG.scheduler == 'ReduceLROnPlateau':\n",
        "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,\n",
        "                                                   mode='min',\n",
        "                                                   factor=0.1,\n",
        "                                                   patience=7,\n",
        "                                                   threshold=0.0001,\n",
        "                                                   min_lr=CFG.min_lr,)\n",
        "    elif CFG.scheduer == 'ExponentialLR':\n",
        "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.85)\n",
        "    elif CFG.scheduler == None:\n",
        "        return None\n",
        "\n",
        "    return scheduler"
      ],
      "metadata": {
        "id": "fz-5zRe-LDx-"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "batch_size = CFG.train_bs\n",
        "\n",
        "my_dataset = MyDataset(X1_scaled_tensor, x2_scaled_tensor, y_tensor)\n",
        "kfold = KFold(n_splits=CFG.n_fold, random_state=99, shuffle=True)\n",
        "model = LSTMScalarPredictor(CFG.lstm_input_size, CFG.lstm_hidden_size, CFG.fc_hidden_size)\n",
        "# 重みとバイアスの初期化\n",
        "model.init_wandb()\n",
        "\n",
        "for fold, (train_ids, valid_ids) in enumerate(kfold.split(my_dataset)):\n",
        "    # Print\n",
        "    print(f'FOLD {fold}')\n",
        "    print('--------------------------------')\n",
        "\n",
        "    # Sample elements randomly from a given list of ids, no replacement.\n",
        "    train_dataset = torch.utils.data.dataset.Subset(my_dataset, train_ids)\n",
        "    valid_dataset = torch.utils.data.dataset.Subset(my_dataset, valid_ids)\n",
        "\n",
        "    # Define data loaders for training and testing data in this fold\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "                      train_dataset,\n",
        "                      batch_size=batch_size)\n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "                      valid_dataset,\n",
        "                      batch_size=batch_size)\n",
        "\n",
        "    run = wandb.init(project='internal-competition',\n",
        "                      config={k:v for k, v in dict(vars(CFG)).items() if '__' not in k},\n",
        "                      anonymous=anonymous,\n",
        "                      name=f\"fold-{fold}|model-{CFG.model_name}\",\n",
        "                      group=CFG.exp_name,\n",
        "                    )\n",
        "\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.wd)\n",
        "    scheduler = fetch_scheduler(optimizer)\n",
        "    model, history = run_training(model, optimizer, scheduler,\n",
        "                                  device=CFG.device,\n",
        "                                  num_epochs=CFG.epochs)\n",
        "    run.finish()"
      ],
      "metadata": {
        "id": "uQ4PQ92FuCu8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "20b666355c5f4d9db77d5bbb2f1830fc",
            "34446a27b1f34093a3618c2ff8c2cc46",
            "9d953ad107ff494ba2bace79c1bce26a",
            "33918e9151b946db82bb15263809636c",
            "7ed2dc4d7e134b21b3f284dd563e277a",
            "d59a116b87774435828bf110ccda7ee7",
            "36713b15649f4b6395f1f8732ce6151f",
            "0f5aae332bbd4786bd6288b797e5d658"
          ]
        },
        "outputId": "30a68076-8980-4d07-d009-d26873cb6be7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOLD 0\n",
            "--------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:q5u5t1ab) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.010 MB of 0.010 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "20b666355c5f4d9db77d5bbb2f1830fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>LR</td><td>█▁</td></tr><tr><td>Train Loss</td><td>█▁</td></tr><tr><td>Valid Loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Epoch</td><td>2</td></tr><tr><td>Best Loss</td><td>1.11625</td></tr><tr><td>LR</td><td>0.0002</td></tr><tr><td>Train Loss</td><td>1.17086</td></tr><tr><td>Valid Loss</td><td>1.11625</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold-0|model-LSTM-FC</strong> at: <a href='https://wandb.ai/nenecchicv/internal-competition/runs/q5u5t1ab' target=\"_blank\">https://wandb.ai/nenecchicv/internal-competition/runs/q5u5t1ab</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240311_123543-q5u5t1ab/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:q5u5t1ab). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240311_124243-3clvvyg9</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nenecchicv/internal-competition/runs/3clvvyg9' target=\"_blank\">fold-0|model-LSTM-FC</a></strong> to <a href='https://wandb.ai/nenecchicv/internal-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nenecchicv/internal-competition' target=\"_blank\">https://wandb.ai/nenecchicv/internal-competition</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nenecchicv/internal-competition/runs/3clvvyg9' target=\"_blank\">https://wandb.ai/nenecchicv/internal-competition/runs/3clvvyg9</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train :   0%|          | 0/2000 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:1345: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
            "  warnings.warn(warn_msg)\n",
            "Train : 100%|██████████| 2000/2000 [00:25<00:00, 77.55it/s, gpu_mem=0.00 GB, lr=0.00020, train_loss=1.2059]\n",
            "Valid : 100%|██████████| 500/500 [00:02<00:00, 172.94it/s, gpu_memory=0.00 GB, lr=0.00020, valid_loss=0.7475]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mValid Score Improved (inf ---> 0.7475)\n",
            "Model Saved\u001b[0m\n",
            "\n",
            "\n",
            "Epoch 2/200"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train : 100%|██████████| 2000/2000 [00:25<00:00, 78.25it/s, gpu_mem=0.00 GB, lr=0.00020, train_loss=0.8024]\n",
            "Valid : 100%|██████████| 500/500 [00:03<00:00, 147.49it/s, gpu_memory=0.00 GB, lr=0.00020, valid_loss=0.6607]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mValid Score Improved (0.7475 ---> 0.6607)\n",
            "Model Saved\u001b[0m\n",
            "\n",
            "\n",
            "Epoch 3/200"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train : 100%|██████████| 2000/2000 [00:25<00:00, 78.09it/s, gpu_mem=0.00 GB, lr=0.00020, train_loss=0.6457]\n",
            "Valid : 100%|██████████| 500/500 [00:03<00:00, 147.49it/s, gpu_memory=0.00 GB, lr=0.00020, valid_loss=0.6739]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Epoch 4/200"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train : 100%|██████████| 2000/2000 [00:25<00:00, 77.82it/s, gpu_mem=0.00 GB, lr=0.00020, train_loss=0.5715]\n",
            "Valid : 100%|██████████| 500/500 [00:03<00:00, 136.82it/s, gpu_memory=0.00 GB, lr=0.00020, valid_loss=0.7023]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Epoch 5/200"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train : 100%|██████████| 2000/2000 [00:25<00:00, 77.88it/s, gpu_mem=0.00 GB, lr=0.00020, train_loss=0.5364]\n",
            "Valid : 100%|██████████| 500/500 [00:03<00:00, 128.79it/s, gpu_memory=0.00 GB, lr=0.00020, valid_loss=0.6764]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Epoch 6/200"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train : 100%|██████████| 2000/2000 [00:26<00:00, 75.10it/s, gpu_mem=0.00 GB, lr=0.00020, train_loss=0.5077]\n",
            "Valid : 100%|██████████| 500/500 [00:03<00:00, 137.42it/s, gpu_memory=0.00 GB, lr=0.00020, valid_loss=0.6581]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mValid Score Improved (0.6607 ---> 0.6581)\n",
            "Model Saved\u001b[0m\n",
            "\n",
            "\n",
            "Epoch 7/200"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train : 100%|██████████| 2000/2000 [00:25<00:00, 79.22it/s, gpu_mem=0.00 GB, lr=0.00020, train_loss=0.4922]\n",
            "Valid : 100%|██████████| 500/500 [00:03<00:00, 153.77it/s, gpu_memory=0.00 GB, lr=0.00020, valid_loss=0.6750]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Epoch 8/200"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train : 100%|██████████| 2000/2000 [00:24<00:00, 80.39it/s, gpu_mem=0.00 GB, lr=0.00020, train_loss=0.4743]\n",
            "Valid : 100%|██████████| 500/500 [00:03<00:00, 146.09it/s, gpu_memory=0.00 GB, lr=0.00020, valid_loss=0.6614]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Epoch 9/200"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train : 100%|██████████| 2000/2000 [00:24<00:00, 80.95it/s, gpu_mem=0.00 GB, lr=0.00020, train_loss=0.4620]\n",
            "Valid : 100%|██████████| 500/500 [00:03<00:00, 163.12it/s, gpu_memory=0.00 GB, lr=0.00020, valid_loss=0.6615]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Epoch 10/200"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train : 100%|██████████| 2000/2000 [00:24<00:00, 83.03it/s, gpu_mem=0.00 GB, lr=0.00020, train_loss=0.4509]\n",
            "Valid : 100%|██████████| 500/500 [00:03<00:00, 131.83it/s, gpu_memory=0.00 GB, lr=0.00020, valid_loss=0.6520]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mValid Score Improved (0.6581 ---> 0.6520)\n",
            "Model Saved\u001b[0m\n",
            "\n",
            "\n",
            "Epoch 11/200"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train : 100%|██████████| 2000/2000 [00:24<00:00, 80.92it/s, gpu_mem=0.00 GB, lr=0.00020, train_loss=0.4422]\n",
            "Valid :  68%|██████▊   | 342/500 [00:02<00:01, 117.50it/s, gpu_memory=0.00 GB, lr=0.00020, valid_loss=0.7883]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X_test = X1_scaled_test_tensor\n",
        "\n",
        "# x2  = torch.full((X1.size(0), 1), 100).to(device, dtype=torch.float)"
      ],
      "metadata": {
        "id": "-dizmPUEwBbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/code/fnands/uwmgi-levit-unet-baseline"
      ],
      "metadata": {
        "id": "R8hSyV-ZdE40"
      }
    }
  ]
}