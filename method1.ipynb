{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/nenecchiCV/internal-competition-2024/blob/main/method1.ipynb",
      "authorship_tag": "ABX9TyODxrLrFbJW2h/rMs7yoYSy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nenecchiCV/internal-competition-2024/blob/main/method1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-vYPlGg3kAB",
        "outputId": "939f0298-7f14-4c7d-b2a7-0f3d7fa28e11"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = \"\""
      ],
      "metadata": {
        "id": "zurfTCmEgwqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "Hy9sI7w5Mg3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "947b989f-fda5-497a-84b9-2be6c0d6a1e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (0.4.6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        }
      ],
      "source": [
        "!pip install colorama\n",
        "!pip install wandb -qqq\n",
        "import wandb\n",
        "\n",
        "try:\n",
        "    wandb.login(key=api_key)\n",
        "    anonymous = None\n",
        "except:\n",
        "    anonymous = \"must\"\n",
        "    print('To use your W&B account,\\nGo to Add-ons -> Secrets and provide your W&B access token. Use the Label name as WANDB. \\nGet your W&B access token from here: https://wandb.ai/authorize')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "data1_df = pd.read_csv('/content/drive/MyDrive/daicc2024/data_1.csv')\n",
        "data2_df = pd.read_csv('/content/drive/MyDrive/daicc2024/data_2.csv')\n",
        "data_df = pd.concat([data1_df, data2_df], axis=0)\n",
        "data_df.head()\n",
        "\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/daicc2024/test.csv')\n",
        "all_df = pd.concat([data_df, test_df], axis=0)"
      ],
      "metadata": {
        "id": "CeK-NL8tdmWi"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X1 = data_df.drop(columns=[\"id\",'parameter',\"spec\"])\n",
        "x2 = data_df[['spec']]\n",
        "y = data_df[['parameter']]\n",
        "\n",
        "X1_all = all_df.drop(columns=[\"id\",'parameter',\"spec\"])\n",
        "x2_min_max = np.array([[90], [110]]).astype('float')\n",
        "x2_min_max_df = pd.DataFrame(data=x2_min_max, columns=[\"spec\"], dtype=float)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import torch\n",
        "\n",
        "# trainというDataFrameにfit\n",
        "X1_scaler = StandardScaler().fit(X1_all)\n",
        "x2_scaler = MinMaxScaler(feature_range=(-1, 1)).fit(x2_min_max_df)\n",
        "X1_scaled = X1_scaler.transform(X1)\n",
        "x2_scaled = x2_scaler.transform(x2)\n",
        "\n",
        "X1_scaled_tensor = torch.tensor(X1_scaled.reshape(-1, 3, 5), dtype=torch.float64)\n",
        "x2_scaled_tensor = torch.tensor(x2_scaled, dtype=torch.float64)\n",
        "y_tensor = torch.tensor(y.values, dtype=torch.float64)\n",
        "\n",
        "X1_test = test_df.drop(columns=[\"id\"])\n",
        "X1_scaled_test = X1_scaler.transform(X1_test)\n",
        "X1_scaled_test_tensor = torch.tensor(X1_scaled_test.reshape(-1, 3, 5), dtype=torch.float64)"
      ],
      "metadata": {
        "id": "9bxQcjEDgzz8"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "メモ：計測値とスコアをX、パラメータをYとして学習→結果を出すときはスコアを100にする\n",
        "\n",
        "RNN\n",
        "https://qiita.com/MENDY/items/99da56f61f9af51dda15"
      ],
      "metadata": {
        "id": "2PI5v9kKGwq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils import data as data\n",
        "\n",
        "class MyDataset(data.Dataset):\n",
        "    def __init__(self, X1_scaled_tensor, x2_scaled_tensor, y_tensor):\n",
        "        self.X1 = X1_scaled_tensor\n",
        "        self.x2 = x2_scaled_tensor\n",
        "        self.y = y_tensor\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.X1[index], self.x2[index], self.y[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)"
      ],
      "metadata": {
        "id": "xFxsPwmM3oCz"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "\n",
        "class CFG:\n",
        "    seed = 99\n",
        "    exp_name = 'exp_1'\n",
        "    comment = exp_name\n",
        "    model_name = 'LSTM-FC'\n",
        "    train_bs = 32\n",
        "    valid_bs = train_bs\n",
        "    data_size = [[3, 5], [1]]\n",
        "    epochs = 100\n",
        "    lr = 2e-4\n",
        "    scheduler = 'CosineAnnealingLR'\n",
        "    min_lr = 1e-6\n",
        "    T_max = int(30000 / train_bs * epochs) + 50\n",
        "    T_0 = 25\n",
        "    warmup_epochs = 0\n",
        "    wd = 1e-6\n",
        "    n_fold = 5\n",
        "    folds = []\n",
        "    device = \"cpu\"\n",
        "    # device = xm.xla_device()\n",
        "    lstm_input_size=5\n",
        "    lstm_hidden_size=10\n",
        "    fc_hidden_size=256\n",
        "    dropout_rate = 0.4\n",
        "#     target_dataset='uwmgi-stride2-dataset/images'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "91xKiQmx88ad",
        "outputId": "bd937670-d374-466d-9fab-1444ea166705"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch_xla'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-432b5ee37898>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_xla\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_xla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxla_model\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m99\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_xla'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "class LSTMPredictor(nn.Module):\n",
        "    def __init__(self, lstm_input_size=CFG.lstm_input_size, lstm_hidden_size=CFG.lstm_hidden_size, fc_hidden_size=CFG.fc_hidden_size):\n",
        "        super(LSTMPredictor, self).__init__()\n",
        "        self.lstm_hidden_size = lstm_hidden_size\n",
        "        self.lstm = nn.LSTM(lstm_input_size, lstm_hidden_size, batch_first=True)\n",
        "        self.fc1 = nn.Linear(lstm_hidden_size + 1, fc_hidden_size)  # +1 for scalar input\n",
        "        self.fc2 = nn.Linear(fc_hidden_size, fc_hidden_size)\n",
        "        self.fc3 = nn.Linear(fc_hidden_size, 1)\n",
        "        self.dropout1 = nn.Dropout2d(CFG.dropout_rate)\n",
        "        self.dropout2 = nn.Dropout2d(CFG.dropout_rate)\n",
        "        self.dropout3 = nn.Dropout2d(CFG.dropout_rate)\n",
        "        self.bn1 = nn.BatchNorm1d(fc_hidden_size)\n",
        "        self.bn2 = nn.BatchNorm1d(fc_hidden_size)\n",
        "\n",
        "    def forward(self, X1, x2):\n",
        "        # Initialize hidden state with zeros\n",
        "        h0 = torch.zeros(1, X1.size(0), self.lstm_hidden_size).to(X1.device)\n",
        "        c0 = torch.zeros(1, X1.size(0), self.lstm_hidden_size).to(X1.device)\n",
        "\n",
        "        # Forward pass through LSTM layer\n",
        "        lstm_H, _ = self.lstm(X1, (h0, c0))\n",
        "\n",
        "        # Only take the output from the last time step\n",
        "        x = torch.cat([lstm_H[:, -1, :], x2], dim=1)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout3(x)\n",
        "        out = self.fc3(x)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "IEHjxyGTLrpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MSELoss = nn.MSELoss()\n",
        "def criterion(y_pred, y_true):\n",
        "    return MSELoss(y_pred, y_true)"
      ],
      "metadata": {
        "id": "aA-i_sAAMklM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import os\n",
        "def set_seed(seed = 42):\n",
        "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
        "    This is for REPRODUCIBILITY.'''\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # When running on the CuDNN backend, two further options must be set\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    # Set a fixed value for the hash seed\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    print('> SEEDING DONE')\n",
        "\n",
        "set_seed(CFG.seed)"
      ],
      "metadata": {
        "id": "7yMWT0eH8Z-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import gc\n",
        "\n",
        "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
        "    model.train()\n",
        "\n",
        "    dataset_size = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Train ')\n",
        "    for step, (X1, x2, y_true) in pbar:\n",
        "        X1 = X1.to(device, dtype=torch.float)\n",
        "        x2  = x2.to(device, dtype=torch.float)\n",
        "        y_true  = y_true.to(device, dtype=torch.float)\n",
        "\n",
        "        batch_size = X1.size(0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_pred = model(X1, x2)\n",
        "        loss = criterion(y_pred, y_true)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        running_loss += (loss.item() * batch_size)\n",
        "        dataset_size += batch_size\n",
        "\n",
        "        epoch_loss = running_loss / dataset_size\n",
        "\n",
        "        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        pbar.set_postfix(train_loss=f'{epoch_loss:0.4f}',\n",
        "                        lr=f'{current_lr:0.5f}',\n",
        "                        gpu_mem=f'{mem:0.2f} GB')\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return epoch_loss\n",
        "\n",
        "@torch.no_grad()\n",
        "def valid_one_epoch(model, dataloader, device, epoch):\n",
        "    model.eval()\n",
        "\n",
        "    dataset_size = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    val_scores = []\n",
        "\n",
        "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Valid ')\n",
        "    for step, (X1, x2, y_true) in pbar:\n",
        "        X1 = X1.to(device, dtype=torch.float)\n",
        "        x2 = x2.to(device, dtype=torch.float)\n",
        "        y_true  = y_true.to(device, dtype=torch.float)\n",
        "\n",
        "        batch_size = X1.size(0)\n",
        "\n",
        "        y_pred = model(X1, x2)\n",
        "        loss = criterion(y_pred, y_true)\n",
        "\n",
        "        running_loss += (loss.item() * batch_size)\n",
        "        dataset_size += batch_size\n",
        "\n",
        "        epoch_loss = running_loss / dataset_size\n",
        "\n",
        "        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        pbar.set_postfix(valid_loss=f'{epoch_loss:0.4f}',\n",
        "                        lr=f'{current_lr:0.5f}',\n",
        "                        gpu_memory=f'{mem:0.2f} GB')\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return epoch_loss"
      ],
      "metadata": {
        "id": "w70KIUNFKALD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "from collections import defaultdict\n",
        "import time\n",
        "from colorama import Fore, Back, Style\n",
        "c_  = Fore.GREEN\n",
        "sr_ = Style.RESET_ALL\n",
        "\n",
        "def run_training(model, optimizer, scheduler, device, num_epochs):\n",
        "    # To automatically log gradients\n",
        "    wandb.watch(model, log_freq=100)\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"cuda: {}\\n\".format(torch.cuda.get_device_name()))\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss      = np.inf\n",
        "    best_epoch     = -1\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        gc.collect()\n",
        "        print(f'Epoch {epoch}/{num_epochs}', end='')\n",
        "        train_loss = train_one_epoch(model, optimizer, scheduler,\n",
        "                                           dataloader=train_loader,\n",
        "                                           device=CFG.device, epoch=epoch)\n",
        "\n",
        "        val_loss = valid_one_epoch(model, valid_loader,\n",
        "                                                 device=CFG.device,\n",
        "                                                 epoch=epoch)\n",
        "\n",
        "        history['Train Loss'].append(train_loss)\n",
        "        history['Valid Loss'].append(val_loss)\n",
        "\n",
        "        # Log the metrics\n",
        "        wandb.log({\"Train Loss\": train_loss,\n",
        "                   \"Valid Loss\": val_loss,\n",
        "                   \"LR\":scheduler.get_last_lr()[0]})\n",
        "\n",
        "        if val_loss < best_loss:\n",
        "            print(f\"{c_}Valid Score Improved ({best_loss:0.4f} ---> {val_loss:0.4f})\")\n",
        "            best_loss = val_loss\n",
        "            best_epoch   = epoch\n",
        "            run.summary[\"Best Loss\"]    = best_loss\n",
        "            run.summary[\"Best Epoch\"]   = best_epoch\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            PATH = f\"best_epoch-{fold:02d}.bin\"\n",
        "            torch.save(model.state_dict(), PATH)\n",
        "            # Save a model file from the current directory\n",
        "            wandb.save(PATH)\n",
        "            print(f\"Model Saved{sr_}\")\n",
        "\n",
        "\n",
        "        print(); print()\n",
        "\n",
        "    end = time.time()\n",
        "    time_elapsed = end - start\n",
        "    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
        "    print(\"Best Loss: {:.4f}\".format(best_loss))\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "xUe04P51LcW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_scheduler(optimizer):\n",
        "    if CFG.scheduler == 'CosineAnnealingLR':\n",
        "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CFG.T_max,\n",
        "                                                   eta_min=CFG.min_lr)\n",
        "    elif CFG.scheduler == 'CosineAnnealingWarmRestarts':\n",
        "        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CFG.T_0,\n",
        "                                                             eta_min=CFG.min_lr)\n",
        "    elif CFG.scheduler == 'ReduceLROnPlateau':\n",
        "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,\n",
        "                                                   mode='min',\n",
        "                                                   factor=0.1,\n",
        "                                                   patience=7,\n",
        "                                                   threshold=0.0001,\n",
        "                                                   min_lr=CFG.min_lr,)\n",
        "    elif CFG.scheduer == 'ExponentialLR':\n",
        "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.85)\n",
        "    elif CFG.scheduler == None:\n",
        "        return None\n",
        "\n",
        "    return scheduler"
      ],
      "metadata": {
        "id": "fz-5zRe-LDx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "# モデルのパラメータ\n",
        "input_size = 5  # 入力の次元数\n",
        "hidden_size = 10  # 隠れ層の次元数\n",
        "output_size = 1  # 出力の次元数（スカラー値）\n",
        "\n",
        "batch_size = 10\n",
        "seq_length = 3\n",
        "dummy_input = torch.randn(batch_size, seq_length, input_size)\n",
        "scalar_value = torch.tensor([[42.0]])  # 任意のスカラー値\n",
        "\n",
        "my_dataset = MyDataset(X1_scaled_tensor, x2_scaled_tensor, y_tensor)\n",
        "kfold = KFold(n_splits=CFG.n_fold, random_state=99, shuffle=True)\n",
        "model = LSTMPredictor(input_size, hidden_size, output_size)\n",
        "for fold, (train_ids, valid_ids) in enumerate(kfold.split(my_dataset)):\n",
        "    # Print\n",
        "    print(f'FOLD {fold}')\n",
        "    print('--------------------------------')\n",
        "\n",
        "    # Sample elements randomly from a given list of ids, no replacement.\n",
        "    train_dataset = torch.utils.data.dataset.Subset(my_dataset, train_ids)\n",
        "    valid_dataset = torch.utils.data.dataset.Subset(my_dataset, valid_ids)\n",
        "\n",
        "    # Define data loaders for training and testing data in this fold\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "                      train_dataset,\n",
        "                      batch_size=batch_size)\n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "                      valid_dataset,\n",
        "                      batch_size=batch_size)\n",
        "\n",
        "    run = wandb.init(project='internal-competition',\n",
        "                      config={k:v for k, v in dict(vars(CFG)).items() if '__' not in k},\n",
        "                      anonymous=anonymous,\n",
        "                      name=f\"fold-{fold}|model-{CFG.model_name}\",\n",
        "                      group=CFG.exp_name,\n",
        "                    )\n",
        "\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.wd)\n",
        "    scheduler = fetch_scheduler(optimizer)\n",
        "    model, history = run_training(model, optimizer, scheduler,\n",
        "                                  device=CFG.device,\n",
        "                                  num_epochs=CFG.epochs)\n",
        "    run.finish()"
      ],
      "metadata": {
        "id": "uQ4PQ92FuCu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_test = X1_scaled_test_tensor\n",
        "\n",
        "# x2  = torch.full((X1.size(0), 1), 100).to(device, dtype=torch.float)"
      ],
      "metadata": {
        "id": "-dizmPUEwBbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/code/fnands/uwmgi-levit-unet-baseline"
      ],
      "metadata": {
        "id": "R8hSyV-ZdE40"
      }
    }
  ]
}